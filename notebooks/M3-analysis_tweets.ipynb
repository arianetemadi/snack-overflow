{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline performance on New Data - Tweets\n",
    "----\n",
    "To verify whether our baseline models train to detect sarcasm, we collect a second dataset for sarcasm detection, which is completely unseen data for our models. This new dataset contains short-form content much like the headlines. However, unlike headlines, tweets aren't written by professionals but by individuals expressing their feelings and opinions in an informal style. Moreover, the topics might differ as well since tweets are not necessarily for reflecting on current news affairs. \n",
    "\n",
    "Despite that, we opted to test our best-performing baseline model, the Naive Bayes classifier with the uni-, bi-, and trigrams, to analyze the behavior and compare the performance metrics to the test set of the headlines data we trained on. \n",
    "\n",
    "The main motivation for adding an extra dataset was also the observation that the Onion headlines (represent the sarcastic class) have a very specific style of writing that could be argued that is rather funny but not necassarily sarcastic in all cases, and since the representatives of the `sarcastic` class come only from the Onion headlines we wondered how much of it comes down to their specific style rather than sarcasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from src.data_util import load_data\n",
    "from src.naive_bayes import NaiveBayesClassifier\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines for training, validation,         and test is 20033, 4293,         and 4293 resp.\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "headlines = load_data(\"../data/headline_data/headlines.conllu\")\n",
    "\n",
    "# split into training and test sets\n",
    "SEED = 42\n",
    "train_headlines, other_headlines = split(headlines, test_size=0.3, random_state=SEED)\n",
    "val_headlines, test_headlines = split(other_headlines, test_size=0.5, random_state=SEED)\n",
    "print(\n",
    "    f\"Number of headlines for training, validation, \\\n",
    "        and test is {len(train_headlines)}, {len(val_headlines)}, \\\n",
    "        and {len(test_headlines)} resp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSC\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [05:31<00:00, 60.41it/s]\n",
      "100%|██████████| 20033/20033 [01:29<00:00, 224.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       1.00      1.00      1.00     10530\n",
      "    Sarcastic       1.00      1.00      1.00      9503\n",
      "\n",
      "     accuracy                           1.00     20033\n",
      "    macro avg       1.00      1.00      1.00     20033\n",
      " weighted avg       1.00      1.00      1.00     20033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:16<00:00, 266.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.89      0.86      2237\n",
      "    Sarcastic       0.87      0.81      0.84      2056\n",
      "\n",
      "     accuracy                           0.85      4293\n",
      "    macro avg       0.85      0.85      0.85      4293\n",
      " weighted avg       0.85      0.85      0.85      4293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayesClassifier(ngram_range=(1, 3))\n",
    "naive_bayes.fit(train_headlines)\n",
    "fp, fn = naive_bayes.test(train_headlines)\n",
    "fp, fn = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset\n",
    "tweets = load_data(\"../data/tweets_data/tweets.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sarcastic = [tweet for tweet in tweets if tweet[0].metadata['class'] == \"1\"]\n",
    "tweets_non_sarcastic = [tweet for tweet in tweets if tweet[0].metadata['class'] == \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "867"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2601"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_non_sarcastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenList<The, only, thing, I, got, from, college, is, a, caffeine, addiction, metadata={text: \"The only thing I got from college is a caffeine addiction\", headline_id: \"1\", sent_id: \"0\", class: \"1\"}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_non_sarcastic_sample = []\n",
    "sampled_indices = np.random.choice(len(tweets_non_sarcastic), size=len(tweets_sarcastic), replace=False)\n",
    "for idx in sampled_indices:\n",
    "    tweets_non_sarcastic_sample.append(tweets_non_sarcastic[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1734"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_tweets = tweets_sarcastic + tweets_non_sarcastic_sample\n",
    "len(sampled_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1734/1734 [00:06<00:00, 286.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.50      0.86      0.63       867\n",
      "    Sarcastic       0.51      0.15      0.23       867\n",
      "\n",
      "     accuracy                           0.50      1734\n",
      "    macro avg       0.51      0.50      0.43      1734\n",
      " weighted avg       0.51      0.50      0.43      1734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test on extra data\n",
    "fp, fn = naive_bayes.test(sampled_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes baseline achieved a satisfactory performance on the news headlines, however, with the new dataset the metrics are significantly worse, which might suggest that we are actually not learning to detect sarcasm but rather Onion writing style vs. Huffpost writing style.\n",
    "\n",
    "Although it is hard to define sarcasm exactly, and even though we had trouble detecting it when performing the error analysis after milestone 1, we saw that this might come down to detecting humor instead, as we discussed after our final presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
