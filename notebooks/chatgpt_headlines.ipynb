{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake headlines generated by ChatGPT\n",
    "\n",
    "We asked ChatGPT to generate headlines similar to the style of our two news sources, Onion and HuffPost.\n",
    "We gathered 510 fake headlines for each, 1020 in total. In the process of prompting, we used prompting techniques to get better performance from the model, to make sure the model itself is considering what is the style of each news source and what are their properties. And to make sure the generated headlines are diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gathered all headlines in the text file `data/chatgpt_dataset.txt`.\n",
    "Here are some examples:\n",
    "* Like Onion:\n",
    "    * local dad hopes to be remembered as the guy who never saw ‘the fast and furious’\n",
    "    * area man insists world is flat, despite never leaving his basement\n",
    "    * scientists warn new virus could spread via passive-aggressive texts\n",
    "    * area siblings finally agree to stop faking peace during family gatherings\n",
    "    * survey finds the majority of couples argue over whether to watch tv or just stare into space\n",
    "    * couple celebrates 10 years together by agreeing they still don’t know how to do the dishes\n",
    "    * report finds 68% of workplace meetings could be emails, and 32% could be screams\n",
    "    * new candle company promises scents that smell exactly like regret\n",
    "    * local dog suspiciously eyeing amazon package\n",
    "    * new gym introduces revolutionary workout where you just watch people exercise\n",
    "* Like HuffPost:\n",
    "    * how ai is quietly shaping the future of democracy\n",
    "    * this common food might be sabotaging your weight loss goals\n",
    "    * the heartwarming story of a dog who saved its owner’s life\n",
    "    * why millennials are obsessed with vintage tupperware\n",
    "    * how a group of teens built an app that’s saving lives\n",
    "    * this grandmother’s daily walks are inspiring an entire community\n",
    "    * why experts say you should declutter your mind before your closet\n",
    "    * how tech layoffs are creating new opportunities for start-ups\n",
    "    * how to stop comparing yourself to others – for good\n",
    "    * this single mom’s side hustle turned into a $1 million business\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our opinion, ChatGPT did a reasonable job at faking headlines of these two sources.\n",
    "Many fake headlines for Onion are genuinely funny with a dead-pan tone, and many fake headlines imitating HuffPost are serious, inspirational, and practical.\n",
    "Each seems to follow the style of its respective source.\n",
    "\n",
    "Now let's analyze..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from src.preprocessing import convert_txt_to_json, convert_to_conllu\n",
    "from src.data_util import load_data\n",
    "from src.naive_bayes import NaiveBayesClassifier\n",
    "from src.patterns import fit_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert the data to the conllu format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [00:08<00:00, 123.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert txt file to JSON file\n",
    "input_file = \"../data/chatgpt_onionstyle_data/chatgpt_onionstyle.txt\"\n",
    "output_file = \"../data/chatgpt_onionstyle_data/chatgpt_onionstyle.json\"\n",
    "convert_txt_to_json(\n",
    "    input_file=input_file, output_file=output_file, link=\"https://chatgpt.com/\"\n",
    ")\n",
    "\n",
    "# convert JSON file to conllu dataset\n",
    "output_file = \"../data/chatgpt_onionstyle_data/chatgpt_onionstyle.conllu\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "file_path = \"../data/chatgpt_onionstyle_data/chatgpt_onionstyle.json\"\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "convert_to_conllu(data, output_file, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test our Naive Bayes Bag of Words model on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines for training, testing,         and testing fake headlines is 20033, 4293,         and 1020 resp.\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "headlines = load_data(\"../data/headline_data/headlines.conllu\")\n",
    "fake_headlines = load_data(\"../data/chatgpt_onionstyle_data/chatgpt_onionstyle.conllu\")\n",
    "\n",
    "# split into training and test sets\n",
    "SEED = 42\n",
    "train_headlines, other_headlines = split(headlines, test_size=0.3, random_state=SEED)\n",
    "_, test_headlines = split(other_headlines, test_size=0.5, random_state=SEED)\n",
    "print(\n",
    "    f\"Number of headlines for training, testing, \\\n",
    "        and testing fake headlines is {len(train_headlines)}, {len(test_headlines)}, \\\n",
    "        and {len(fake_headlines)} resp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSC\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [00:35<00:00, 561.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit the Naive Bayes Bag of Word model to training data\n",
    "naive_bayes = NaiveBayesClassifier(ngram_range=(1, 1))\n",
    "naive_bayes.fit(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:02<00:00, 1963.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.86      0.85      2237\n",
      "    Sarcastic       0.84      0.82      0.83      2056\n",
      "\n",
      "     accuracy                           0.84      4293\n",
      "    macro avg       0.84      0.84      0.84      4293\n",
      " weighted avg       0.84      0.84      0.84      4293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test on test data (of the original dataset, not fake)\n",
    "_, _ = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [00:00<00:00, 1409.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.89      0.95      0.92       510\n",
      "    Sarcastic       0.95      0.89      0.92       510\n",
      "\n",
      "     accuracy                           0.92      1020\n",
      "    macro avg       0.92      0.92      0.92      1020\n",
      " weighted avg       0.92      0.92      0.92      1020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# now test on fake headlines generated by ChatGPT and get false positive and negatives\n",
    "fp, fn = naive_bayes.test(fake_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our model that is trained on the original dataset, performs very well on classifying these fake headlines generated by ChatGPT.\n",
    "In fact, it reaches a much higher value for all three metrics (precision, recall, and f1-score) on both classes than it had for our actual test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe this is because ChatGPT is utilizing the most common patterns of each source too much when generating fake headlines.\n",
    "Consider some familiar patterns by Onion, the ones that include expressions like\n",
    "* local man...\n",
    "* nation believes...\n",
    "* study shows...\n",
    "\n",
    "These patterns are abundant in Onion headlines.\n",
    "But by taking a look at the datasets, it becomes clear that ChatGPT is using these patterns too much.\n",
    "The same is true for some familiar patterns used by HuffPost:\n",
    "* how...\n",
    "* why...\n",
    "* here's..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's confirm these hypotheses by running some patterns on each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =  [\n",
    "    \".*area man.*\",\n",
    "    \".*nation.*\",\n",
    "    \".*local.*\",\n",
    "    \".*study.*\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 1: precision=0.7926267281105991, recall=0.08365758754863813\n",
      "-- examples that fit these patterns: \n",
      "trump's explanation for removing sudan from his travel ban is cringeworthy\n",
      "only 12 of trump's 22,450 employees have given a substantial donation to his campaign\n",
      "frustrated employee no longer even trying to hide gre study books\n",
      "local hamburger to star in national ad\n",
      "obese doctors urge nation to eat three meals a meal\n",
      "guy from sopranos drops by local pizza parlor for free slice\n",
      "why the azores are the best european island destination for all types of travelers\n",
      "police investigate reports of local gay man being dragged behind boat\n",
      "fcc assures nation their favorite verizon websites won't be affected by net neutrality repeal\n",
      "only 12 of trump's 22,450 employees have given a substantial donation to his campaign\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(test_headlines, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 1: precision=0.9408866995073891, recall=0.37450980392156863\n",
      "-- examples that fit these patterns: \n",
      "new study finds only 3% of clouds actually have a ‘plan’\n",
      "local cat declares war on ceiling fan, promises total victory\n",
      "study finds cows would eat burgers too, if given the chance\n",
      "new study shows wearing sunglasses instantly increases confidence by 300%\n",
      "nation’s atms begin charging emotional fees for overuse\n",
      "study finds social media influencers now spend 80% of their day influencing themselves\n",
      "area man claims he’s not lost, just ‘exploring a new perspective’\n",
      "nation’s hr departments confirm they still don’t know what hr actually stands for\n",
      "local boss celebrates open-door policy by quietly locking it\n",
      "nation shocked to learn nobody actually knows how to fold a fitted sheet\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(fake_headlines, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these four patterns alone account for more than a third of the fake dataset generated by ChatGPT!\n",
    "The recall on the sarcastic class is about four times, and the precision is also much more!\n",
    "\n",
    "Now let's do the same for some patterns for HuffPost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =  [\n",
    "    \".*why.*\",\n",
    "    \".*how.*\",\n",
    "    \".*here's.*\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 0: precision=0.7516339869281046, recall=0.10281627179257935\n",
      "-- examples that fit these patterns: \n",
      "how to recover from unicorn hair\n",
      "chuck grassley voted against mlk day due to foreseeing how everyone would dishonor king's memory\n",
      "family concerned after aging tv show has another terrible episode\n",
      "why the south carolina church rampage represents a terrorist threat worse than isis\n",
      "why progressives are cautiously optimistic about hillary clinton\n",
      "historical inaccuracy found in wild west strip show\n",
      "trademarks show amazon has sights on meal-kits, 'single cow burgers' and other fast food options\n",
      "barbecue chicken panini succumbs to howard-related causes\n",
      "how donald trump created the worst week any candidate's ever had\n",
      "how pessimism can help you lose weight\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(test_headlines, patterns, label=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 0: precision=0.8615384615384616, recall=0.4392156862745098\n",
      "-- examples that fit these patterns: \n",
      "new reality show features people arguing over how to interpret a google search\n",
      "how green tea might be the key to longer life\n",
      "how the pandemic changed the way we think about work\n",
      "study shows smart homes just want you to think they’re smarter than you\n",
      "why big tech is facing a reckoning over user privacy\n",
      "parents amazed how long kids can cry over broccoli but not out of legos\n",
      "couple celebrates 10 years together by agreeing they still don’t know how to do the dishes\n",
      "how a group of friends turned a hobby into a successful side hustle\n",
      "why experts say you should declutter your mind before your closet\n",
      "why this tech giant’s latest acquisition could transform the industry\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(fake_headlines, patterns, label=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the same can be seen for the non-sarcastic class.\n",
    "These three very simple patterns account for more than 40 percent of fake headlines by ChatGPT, whereas in the original dataset they were about 10 percent, and with noticeably less precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, one can conclude that ChatGPT is reinforcing the sterotypes and styles regarding the headlines of these two sources, by using their common patterns even more than they do.\n",
    "On the other hand, one can simultaneously conclude that these were the easier patterns to detect, and that ChatGPT has a lot of room for improvement in terms of covering all the styles, not just the more common and easier ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
