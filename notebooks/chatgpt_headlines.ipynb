{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake headlines generated by ChatGPT\n",
    "\n",
    "We asked ChatGPT to generate headlines similar to the style of our two news sources, Onion and HuffPost.\n",
    "We gathered 510 fake headlines for each, 1020 in total. In the process of prompting, we used prompting techniques to get better performance from the model, to make sure the model itself is considering what is the style of each news source and what are their properties. And to make sure the generated headlines are diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gathered all headlines in the text file `data/chatgpt_dataset.txt`.\n",
    "Here are some examples:\n",
    "* Like Onion:\n",
    "    * local dad hopes to be remembered as the guy who never saw ‘the fast and furious’\n",
    "    * area man insists world is flat, despite never leaving his basement\n",
    "    * scientists warn new virus could spread via passive-aggressive texts\n",
    "    * area siblings finally agree to stop faking peace during family gatherings\n",
    "    * survey finds the majority of couples argue over whether to watch tv or just stare into space\n",
    "    * couple celebrates 10 years together by agreeing they still don’t know how to do the dishes\n",
    "    * report finds 68% of workplace meetings could be emails, and 32% could be screams\n",
    "    * new candle company promises scents that smell exactly like regret\n",
    "    * local dog suspiciously eyeing amazon package\n",
    "    * new gym introduces revolutionary workout where you just watch people exercise\n",
    "* Like HuffPost:\n",
    "    * how ai is quietly shaping the future of democracy\n",
    "    * this common food might be sabotaging your weight loss goals\n",
    "    * the heartwarming story of a dog who saved its owner’s life\n",
    "    * why millennials are obsessed with vintage tupperware\n",
    "    * how a group of teens built an app that’s saving lives\n",
    "    * this grandmother’s daily walks are inspiring an entire community\n",
    "    * why experts say you should declutter your mind before your closet\n",
    "    * how tech layoffs are creating new opportunities for start-ups\n",
    "    * how to stop comparing yourself to others – for good\n",
    "    * this single mom’s side hustle turned into a $1 million business\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our opinion, ChatGPT did a reasonable job at faking headlines of these two sources.\n",
    "Many fake headlines for Onion are genuinely funny with a dead-pan tone, and many fake headlines imitating HuffPost are serious, inspirational, and practical.\n",
    "Each seems to follow the style of its respective source.\n",
    "\n",
    "Now let's analyze..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from src.preprocessing import convert_txt_to_json, convert_to_conllu\n",
    "from src.data_util import load_data\n",
    "from src.naive_bayes import NaiveBayesClassifier\n",
    "from src.patterns import fit_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert the data to the conllu format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [00:07<00:00, 130.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert txt file to JSON file\n",
    "input_file = \"../data/chatgpt_dataset.txt\"\n",
    "output_file = \"../data/chatgpt_dataset.json\"\n",
    "convert_txt_to_json(\n",
    "    input_file=input_file, output_file=output_file, link=\"https://chatgpt.com/\"\n",
    ")\n",
    "\n",
    "# convert JSON file to conllu dataset\n",
    "output_file = os.path.join(\"../data\", \"chatgpt_dataset.conllu\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "file_path = \"../data/chatgpt_dataset.json\"\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "convert_to_conllu(data, output_file, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test our Naive Bayes Bag of Words model on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines for training, testing,         and testing fake headlines is 20033, 4293,         and 1020 resp.\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "headlines = load_data(\"../data/dataset.conllu\")\n",
    "fake_headlines = load_data(\"../data/chatgpt_dataset.conllu\")\n",
    "\n",
    "# split into training and test sets\n",
    "SEED = 42\n",
    "train_headlines, other_headlines = split(headlines, test_size=0.3, random_state=SEED)\n",
    "_, test_headlines = split(other_headlines, test_size=0.5, random_state=SEED)\n",
    "print(\n",
    "    f\"Number of headlines for training, testing, \\\n",
    "        and testing fake headlines is {len(train_headlines)}, {len(test_headlines)}, \\\n",
    "        and {len(fake_headlines)} resp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arian/projects/snack-overflow-new/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [00:19<00:00, 1030.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit the Naive Bayes Bag of Word model to training data\n",
    "naive_bayes = NaiveBayesClassifier(ngram_range=(1, 1))\n",
    "naive_bayes.fit(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:01<00:00, 3389.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.86      0.85      2237\n",
      "    Sarcastic       0.84      0.82      0.83      2056\n",
      "\n",
      "     accuracy                           0.84      4293\n",
      "    macro avg       0.84      0.84      0.84      4293\n",
      " weighted avg       0.84      0.84      0.84      4293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on test data (of the original dataset, not fake)\n",
    "_, _ = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1020/1020 [00:00<00:00, 3450.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.89      0.95      0.92       510\n",
      "    Sarcastic       0.95      0.89      0.92       510\n",
      "\n",
      "     accuracy                           0.92      1020\n",
      "    macro avg       0.92      0.92      0.92      1020\n",
      " weighted avg       0.92      0.92      0.92      1020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now test on fake headlines generated by ChatGPT and get false positive and negatives\n",
    "fp, fn = naive_bayes.test(fake_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our model that is trained on the original dataset, performs very well on classifying these fake headlines generated by ChatGPT.\n",
    "In fact, it reaches a much higher value for all three metrics (precision, recall, and f1-score) on both classes than it had for our actual test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe this is because ChatGPT is utilizing the most common patterns of each source too much when generating fake headlines.\n",
    "Consider some familiar patterns by Onion, the ones that include expressions like\n",
    "* local man...\n",
    "* nation believes...\n",
    "* study shows...\n",
    "\n",
    "These patterns are abundant in Onion headlines.\n",
    "But by taking a look at the datasets, it becomes clear that ChatGPT is using these patterns too much.\n",
    "The same is true for some familiar patterns used by HuffPost:\n",
    "* how...\n",
    "* why...\n",
    "* here's..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's confirm these hypotheses by running some patterns on each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =  [\n",
    "    \".*area man.*\",\n",
    "    \".*nation.*\",\n",
    "    \".*local.*\",\n",
    "    \".*study.*\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 1: precision=0.7926267281105991, recall=0.08365758754863813\n",
      "-- examples that fit these patterns: \n",
      "controversial study links e-cigarettes to formaldehyde exposure\n",
      "study finds rising sea levels result of expansive colonization effort by dolphins\n",
      "nation demands more slow-motion footage of running basset hounds\n",
      "trump claims greatest threat facing nation toys coming to life while owner not in room\n",
      "across nation, superstores driving out old-fashioned megamalls\n",
      "area man knows exactly which relatives would be problem if he ever came into money\n",
      "merrick garland kind of uncomfortable with political analysts casually pointing out he'll die relatively soon after nomination\n",
      "nation wishes area man were a creep, but, ugh, he's actually really fucking nice\n",
      "area man treats girlfriend to sumptuous 20-second massage\n",
      "fcc assures nation their favorite verizon websites won't be affected by net neutrality repeal\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(test_headlines, patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 1: precision=0.9408866995073891, recall=0.37450980392156863\n",
      "-- examples that fit these patterns: \n",
      "nation’s college professors quietly agree to never update powerpoint templates\n",
      "why your next vacation should be to a national park\n",
      "local yoga instructor finally reaches zen by giving up on all her goals\n",
      "poll finds majority of americans unsure what to do with hands during national anthem\n",
      "nation’s teachers announce they’re done pretending they understand common core math\n",
      "man buys fire extinguisher, immediately becomes local hero in waiting\n",
      "area man convinced alexa has been gossiping about him\n",
      "local it guy celebrates 20th anniversary of telling people to restart their computers\n",
      "area man starts company that specializes in making people feel uncomfortable on zoom\n",
      "why your next vacation should be to a national park\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(fake_headlines, patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these four patterns alone account for more than a third of the fake dataset generated by ChatGPT!\n",
    "The recall on the sarcastic class is about four times, and the precision is also much more!\n",
    "\n",
    "Now let's do the same for some patterns for HuffPost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =  [\n",
    "    \".*why.*\",\n",
    "    \".*how.*\",\n",
    "    \".*here's.*\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 0: precision=0.7516339869281046, recall=0.10281627179257935\n",
      "-- examples that fit these patterns: \n",
      "why you should use conditioner before getting in the shower\n",
      "how a post-planned parenthood world could look for women\n",
      "this stylish kid will teach you how to wear a suit\n",
      "here's another huge reason to eat a plant-based diet\n",
      "how the traditional nylon toothbrush may be causing your gums to disappear\n",
      "why trump's lies and transgressions appeal to his followers\n",
      "why 'it's the thought that counts' is an outdated phrase\n",
      "hillary clinton steals the show with pitch-perfect cameo in 'song for women 2017'\n",
      "watch: how belgium became a hotbed for terrorism\n",
      "why calls for boycotts always hurt the wrong people\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(test_headlines, patterns, label=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- class = 0: precision=0.8615384615384616, recall=0.4392156862745098\n",
      "-- examples that fit these patterns: \n",
      "how a town in sweden turned abandoned boats into public benches\n",
      "why ai could replace millions of jobs by 2030\n",
      "why fitness experts are saying goodbye to crunches forever\n",
      "how mushrooms could be the key to fighting climate change\n",
      "why women’s rugby is gaining popularity like never before\n",
      "how a bus driver’s kindness changed a young boy’s life\n",
      "why scientists are studying a dormant volcano in iceland\n",
      "how to stop comparing yourself to others – for good\n",
      "why this athlete’s return to the court is one of the most inspirational stories of the year\n",
      "the truth about finding your purpose – and why it’s so hard\n"
     ]
    }
   ],
   "source": [
    "_ = fit_patterns(fake_headlines, patterns, label=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so the same can be seen for the non-sarcastic class.\n",
    "These three very simple patterns account for more than 40 percent of fake headlines by ChatGPT, whereas in the original dataset they were about 10 percent, and with noticeably less precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, one can conclude that ChatGPT is reinforcing the sterotypes and styles regarding the headlines of these two sources, by using their common patterns even more than they do.\n",
    "On the other hand, one can simultaneously conclude that these were the easier patterns to detect, and that ChatGPT has a lot of room for improvement in terms of covering all the styles, not just the more common and easier ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
