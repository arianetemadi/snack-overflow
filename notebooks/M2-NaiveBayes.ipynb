{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Bag of Words\n",
    "\n",
    "Let's start by imports and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from src.data_util import load_data\n",
    "from src.naive_bayes import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of headlines for training, validation, and test is 20033, 4293, and 4293 resp.\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "headlines = load_data(\"../data/dataset.conllu\")\n",
    "\n",
    "# split into training and test sets\n",
    "SEED = 42\n",
    "train_headlines, other_headlines = split(headlines, test_size=0.3, random_state=SEED)\n",
    "val_headlines, test_headlines = split(other_headlines, test_size=0.5, random_state=SEED)\n",
    "print(f\"Number of headlines for training, validation, and test is {len(train_headlines)}, {len(val_headlines)}, and {len(test_headlines)} resp.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our Naive Bayes model from the source file `src/naive_bayes.py` and fit it to our train data.\n",
    "\n",
    "Our model is implemented with `MultinomialNB` from `sklearn.naive_bayes`.\n",
    "Its features can be unigrams, bigrams, trigrams, etc, or any range of them, specified by the argument `ngram_range`.\n",
    "For example, the range `(1, 3)` tells the model to consider all unigrams, bigrams, and trigrams as the features to count.\n",
    "\n",
    "The model first counts all features with the help of `CountVectorizer` from `sklearn.feature_extraction.text`.\n",
    "We count the features in each sentence of a headline separately and then add the counts up, so that no ngram we count spans two different sentences.\n",
    "Then, we pass the feature counts to the Naive Bayes model.\n",
    "Finally, we can get predictions from the model.\n",
    "\n",
    "Let's start by training the simplest model, i.e. only unigrams as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arian/projects/snack-overflow/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [00:11<00:00, 1796.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit the Naive Bayes Bag of Word model to training data\n",
    "naive_bayes = NaiveBayesClassifier(ngram_range=(1, 1))\n",
    "naive_bayes.fit(train_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here are the results on the training set and the test set respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20033/20033 [00:03<00:00, 5208.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.92      0.94      0.93     10530\n",
      "    Sarcastic       0.93      0.91      0.92      9503\n",
      "\n",
      "     accuracy                           0.92     20033\n",
      "    macro avg       0.92      0.92      0.92     20033\n",
      " weighted avg       0.92      0.92      0.92     20033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test on training data\n",
    "fp, fn = naive_bayes.test(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:00<00:00, 5176.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.86      0.85      2237\n",
      "    Sarcastic       0.84      0.83      0.83      2056\n",
      "\n",
      "     accuracy                           0.84      4293\n",
      "    macro avg       0.84      0.84      0.84      4293\n",
      " weighted avg       0.84      0.84      0.84      4293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test on test data and get false positive and false negatives\n",
    "fp, fn = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have around 92% for all metrics on training and around 85% for test.\n",
    "Pretty impressive for such a simple model!\n",
    "Of course, we expect the results will be significantly worse when tested on other datasets.\n",
    "The test set we have is still very similar to the training set, since the whole dataset consists of news headlines of only two sources, each with their own unique style.\n",
    "For the next milestone, we shall test the model on another dataset as well.\n",
    "\n",
    "Now let's see a few false positives and a few false negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Some false negatives ---\n",
      "fist-pumping jared kushner leaves jerusalem embassy refreshed and ready to solve next global crisis\n",
      "australian parliament gathers to discuss dwindling hemsworth reserves\n",
      "desperate hillary to obama: 'next vote wins'\n",
      "supreme court upholds bill of rights in 5-4 decision\n",
      "fat kid just wants to watch you guys play\n",
      "mayor daley's son appointed head of illinois nepotist party\n",
      "fbi panicking after learning encrypted national security communications may have been intercepted by trump administration\n",
      "rookie justice gorsuch assigned to supreme court overnight shift\n",
      "supporters praise trump for upholding traditional american value of supporting murderous dictators for political gain\n",
      "giuliani puts odds of trump-mueller interview at 50-65\n",
      "\n",
      "--- Some false positives ---\n",
      "20 struggles every tall girl knows to be true\n",
      "surge soda is back!\n",
      "cupid cop gave out roses, cards on valentine's day instead of tickets\n",
      "early apple computer sells for almost $1 million at auction\n",
      "airasia search continues but bad weather drives back divers\n",
      "massive filament snakes across sun's surface\n",
      "long-shot push to force senate to confirm merrick garland fails in federal court\n",
      "india's cabinet members lose handily\n",
      "determined cat goes through a lot to wrestle with stuffed tiger\n",
      "rebel grandma sneaks out of care home to get a tattoo\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "print(\"\\n--- Some false negatives ---\")\n",
    "for f in fn[:N]:\n",
    "    print(f[0].metadata[\"text\"])\n",
    "print(\"\\n--- Some false positives ---\")\n",
    "for f in fp[:N]:\n",
    "    print(f[0].metadata[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's investigate what might have caused these by examining model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word     sarcastic non-sarcastic          diff\n",
      "========================================================\n",
      "      giuliani         -9.40         -9.56          0.15\n",
      "          puts         -7.24         -8.02          0.78\n",
      "          odds        -10.61         -9.81         -0.80\n",
      "            of         -3.83         -4.27          0.44\n",
      "        *trump         -6.15         -4.83         -1.31\n",
      "             -         -6.47         -6.17         -0.30\n",
      "       mueller         -8.15         -9.11          0.96\n",
      "     interview         -9.00         -8.04         -0.96\n",
      "            at         -5.52         -5.64          0.12\n",
      "            50         -8.93         -8.26         -0.68\n",
      "             -         -6.47         -6.17         -0.30\n",
      "            65        -10.61         -9.96         -0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes.show_word_weights(fn[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word `trump` has a much larger weight for non-sarcastic labels.\n",
    "This complies with our analysis in milestone 1 regarding the most common lemmas: the word `trump` is way more frequent in non-sarcastic headlines.\n",
    "In fact, it is the most common lemma in non-sarcastic headlines, and it appears about three times as many as the second most frequent word.\n",
    "The lemma `trump` still appears a lot in sarcastic headlines as it is the second most common on that list.\n",
    "However, it appears about four times as many among non-sarcastic.\n",
    "This makes the Bag of Words model have a hard time detecting sarcastic headlines containing this name.\n",
    "The existence of the word `trump` should not have much of an effect on determining whether the headline is sarcastic or not.\n",
    "Therefore, this is a particular problem with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the words that were the most decisive with their weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-3.713402245211233, 'queer'),\n",
       "  (-3.665774196221978, 'huffpost'),\n",
       "  (-3.448709690984151, 'colbert'),\n",
       "  (-3.3861893340028164, 'kardashian'),\n",
       "  (-3.3194979595041456, '2017'),\n",
       "  (-3.3194979595041456, 'jenner'),\n",
       "  (-3.3194979595041456, 'lgbtq'),\n",
       "  (-3.1302559598656163, 'hawaii'),\n",
       "  (-3.0876963454468207, 'roundup'),\n",
       "  (-3.0876963454468207, 'trans')],\n",
       " [(3.26667369535053, 'per'),\n",
       "  (3.343634736486658, 'asshole'),\n",
       "  (3.3800023806575323, 'coworker'),\n",
       "  (3.3800023806575332, 'unable'),\n",
       "  (3.5741583950984896, 'onion'),\n",
       "  (3.808997986175891, 'fucking'),\n",
       "  (3.843287059654523, 'area'),\n",
       "  (3.8979454721923874, 'clearly'),\n",
       "  (4.15867173465564, 'shit'),\n",
       "  (4.454517117746582, 'fuck')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.show_decisive_words(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can see that like the word `trump`, the word `donald` is very decisive in making a headline non-sarcastic.\n",
    "\n",
    "Then, let's move on to the word `three`.\n",
    "This word is a bit decisive for sarcastic headlines, it has a weight difference of 0.61.\n",
    "Our hypothesis is that the number `three` is a good round number when people make-up fake sentences.\n",
    "And at least in the case of our source for sarcastic headlines (Onion), most are fake.\n",
    "Therefore, this word appears a lot.\n",
    "However, unlike the previous word, this does not seem to be an artifact.\n",
    "This seems to be a genuine indication of something sarcastic (something fake).\n",
    "Let's take a look at some cases where this word was used in a sarcastic headline:\n",
    "\n",
    "* area dad figures he's got at least *three* more months of screwing around before son gains ability to form long-term memories\n",
    "* salad rendered unhealthy in *three* steps\n",
    "* '97 neons to come in *three* hideous new colors\n",
    "* presence of *three* round objects triggers juggling reflex in local man\n",
    "* man always *three* ingredients away from making pancakes\n",
    "\n",
    "It makes sense that in reality, we would see fewer occurences of this number.\n",
    "When trying to compose a fake sentence, often the number `three` is the best:\n",
    "*three* more months, *three* steps, *three* colors, *three* objects, *three* ingredients.\n",
    "`two` might be too few, and `four` and above can feel like too many.\n",
    "\n",
    "Moving on, we have other words that are more frequently used in sarcastic headlines, like the words `fuck` and `shit`.\n",
    "\n",
    "And then, there are words like `nation` that are used frequently by the source Onion in sarcastic headlines.\n",
    "In fact, `nation` is the most common word in our sarcastic headlines.\n",
    "It is frequently used in this style of their sarcastic headlines:\n",
    "\n",
    "* *nation*'s sane people to *nation*'s insane people: 'please stop shooting us'\n",
    "* *nation*'s poor bastards never even saw it coming\n",
    "* backup health care plan involves *nation* sharing one big jar of ointment\n",
    "* report: north dakota leads *nation* in parking availability\n",
    "* *nation*'s shark experts: 'you could've had this job'\n",
    "\n",
    "Similar joke patterns can be seen with some other words, e.g. with `dad`, `study`, and `local`:\n",
    "* *study*: retired *dads* busier than ever\n",
    "* tech is the future, reports *local* *dad*\n",
    "* area *dad* off to bad start with waitress\n",
    "* groundbreaking *study* finds gratification can be deliberately postponed\n",
    "* congress votes to intervene in *local* wedding\n",
    "* *local* grandmother feared dead after appearing in woman's profile picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the model again, this time with bigrams as features that we count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arian/projects/snack-overflow/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [00:45<00:00, 435.85it/s]\n",
      "100%|██████████| 20033/20033 [00:16<00:00, 1238.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.99      1.00      1.00     10530\n",
      "    Sarcastic       1.00      0.99      0.99      9503\n",
      "\n",
      "     accuracy                           1.00     20033\n",
      "    macro avg       1.00      1.00      1.00     20033\n",
      " weighted avg       1.00      1.00      1.00     20033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:03<00:00, 1253.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.76      0.87      0.81      2237\n",
      "    Sarcastic       0.83      0.70      0.76      2056\n",
      "\n",
      "     accuracy                           0.79      4293\n",
      "    macro avg       0.80      0.79      0.79      4293\n",
      " weighted avg       0.80      0.79      0.79      4293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayesClassifier(ngram_range=(2, 2))\n",
    "naive_bayes.fit(train_headlines)\n",
    "fp, fn = naive_bayes.test(train_headlines)\n",
    "fp, fn = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's run the model with unigrams, bigrams, and trigrams, all as the features that we count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arian/projects/snack-overflow/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 20033/20033 [01:55<00:00, 173.36it/s]\n",
      "100%|██████████| 20033/20033 [00:38<00:00, 513.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       1.00      1.00      1.00     10530\n",
      "    Sarcastic       1.00      1.00      1.00      9503\n",
      "\n",
      "     accuracy                           1.00     20033\n",
      "    macro avg       1.00      1.00      1.00     20033\n",
      " weighted avg       1.00      1.00      1.00     20033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4293/4293 [00:08<00:00, 499.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.89      0.86      2237\n",
      "    Sarcastic       0.87      0.81      0.84      2056\n",
      "\n",
      "     accuracy                           0.85      4293\n",
      "    macro avg       0.85      0.85      0.85      4293\n",
      " weighted avg       0.85      0.85      0.85      4293\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayesClassifier(ngram_range=(1, 3))\n",
    "naive_bayes.fit(train_headlines)\n",
    "fp, fn = naive_bayes.test(train_headlines)\n",
    "fp, fn = naive_bayes.test(test_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both of these cases overfit the training dataset almost completely and the model's decision making becomes perhaps completely reliant on the artifacts in our small dataset.\n",
    "On the test set, bigrams alone perform worse than what we saw at the beginning with unigrams.\n",
    "However, all three ngrams together, make for an ever so slightly improved performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
