{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from src.data_util import load_data\n",
    "from src.naive_bayes import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "3619\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "headlines = load_data(\"../data/dataset.conllu\")\n",
    "\n",
    "# split into training and test sets\n",
    "train_headlines, test_headlines = headlines[:25000], headlines[25000:]\n",
    "print(len(train_headlines))\n",
    "print(len(test_headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arian/projects/snack-overflow/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "100%|██████████| 25000/25000 [00:13<00:00, 1842.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit the Naive Bayes Bag of Word model to training data\n",
    "nb = NaiveBayesClassifier((1, 1))\n",
    "nb.fit(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:05<00:00, 4649.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.92      0.93      0.92     13089\n",
      "    Sarcastic       0.92      0.91      0.91     11911\n",
      "\n",
      "     accuracy                           0.92     25000\n",
      "    macro avg       0.92      0.92      0.92     25000\n",
      " weighted avg       0.92      0.92      0.92     25000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test on training data\n",
    "fp, fn = nb.test(train_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3619/3619 [00:00<00:00, 4466.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-sarcastic       0.84      0.88      0.86      1896\n",
      "    Sarcastic       0.86      0.82      0.84      1723\n",
      "\n",
      "     accuracy                           0.85      3619\n",
      "    macro avg       0.85      0.85      0.85      3619\n",
      " weighted avg       0.85      0.85      0.85      3619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on test data and get false positive and false negatives\n",
    "fp, fn = nb.test(test_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- some false negatives ---\n",
      "new 'cut off your genitals' challenge gains popularity among teens online\n",
      "breaking: we might be doing a bad job\n",
      "well known gresham, or musicians form gresham, or supergroup\n",
      "federal judge pencils blocking trump's unconstitutional executive orders into monthly schedule\n",
      "sight of 400 war elephants on horizon marks hillary clinton's arrival in swing state\n",
      "afro-disney plans scrapped\n",
      "donald trump jr. divorce leaves confused, heartbroken nation wondering why bad things happen to good people\n",
      "new honda commercial openly says your kids will die in a car crash if you buy a different brand\n",
      "custom fireplace store totally jumps gentrification gun\n",
      "diorama of rome built in a day\n",
      "\n",
      "--- some false positives ---\n",
      "gutters and castles\n",
      "bts proves k-pop's power with spot on time magazine's most influential list\n",
      "meryl streep and tom hanks have too much fun playing each other's characters\n",
      "video shows e-cigarette suddenly explode in new jersey woman's handbag\n",
      "john kerry attempts to bully codepink into silence\n",
      "cvs is reportedly in talks to acquire aetna\n",
      "bush's 'electability' argument is getting even weaker\n",
      "nasa joins twitter users to name those newly discovered planets.\n",
      "mitt romney: 'we've gotta rethink campaign finance'\n",
      "all-female rock band reminds moms they are 'enough'\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "print(\"\\n--- some false negatives ---\")\n",
    "for f in fn[:N]:\n",
    "    print(f[0].metadata[\"text\"])\n",
    "print(\"\\n--- some false positives ---\")\n",
    "for f in fp[:N]:\n",
    "    print(f[0].metadata[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_weights(headline):\n",
    "    print(('{:>14}'*4).format('word', 'sarcastic', 'non-sarcastic', 'diff'))\n",
    "    print('='*56)\n",
    "    threshold = 1\n",
    "    for sentence in headline:\n",
    "        for token in sentence:\n",
    "            if token[\"lemma\"] in vocabulary:\n",
    "                neg_weight = bow.feature_log_prob_[0][vectorizer.transform([[token[\"lemma\"]]]).nonzero()[1][0]]\n",
    "                pos_weight = bow.feature_log_prob_[1][vectorizer.transform([[token[\"lemma\"]]]).nonzero()[1][0]]\n",
    "                diff = pos_weight - neg_weight\n",
    "            else:\n",
    "                pos_weight, neg_weight, diff = -1, -1, 0\n",
    "            p_token = token[\"form\"] if abs(diff) < threshold else f\"*{token['form']}\"\n",
    "            print(f'{p_token:>14}{pos_weight:>14.2f}{neg_weight:>14.2f}{diff:>14.2f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word     sarcastic non-sarcastic          diff\n",
      "========================================================\n",
      "         *mitt        -10.76        -11.88          1.12\n",
      "       *romney         -9.40        -11.29          1.90\n",
      "             :         -1.00         -1.00          0.00\n",
      "             '         -7.24         -6.71         -0.53\n",
      "           *we         -9.07         -7.57         -1.50\n",
      "           've         -6.78         -7.14          0.35\n",
      "           got         -7.33         -7.50          0.17\n",
      "            ta         -5.09         -5.31          0.23\n",
      "       rethink        -12.09        -12.11          0.02\n",
      "      campaign         -8.75         -9.03          0.27\n",
      "       finance        -11.86        -11.55         -0.32\n",
      "             '         -7.24         -6.71         -0.53\n",
      "\n",
      "          word     sarcastic non-sarcastic          diff\n",
      "========================================================\n",
      "        *sight        -10.83        -11.88          1.05\n",
      "            of         -5.36         -5.82          0.46\n",
      "           400        -10.91        -11.88          0.97\n",
      "           war         -9.03         -8.70         -0.33\n",
      "     elephants        -11.86        -11.41         -0.45\n",
      "            on         -6.44         -6.43         -0.01\n",
      "      *horizon        -13.47        -12.39         -1.08\n",
      "         marks         -9.98         -9.64         -0.33\n",
      "      *hillary         -9.81         -8.52         -1.29\n",
      "       clinton         -8.64         -8.30         -0.34\n",
      "            's         -6.17         -5.83         -0.34\n",
      "       arrival        -11.86        -11.88          0.02\n",
      "            in         -5.94         -5.83         -0.12\n",
      "         swing        -11.68        -12.11          0.42\n",
      "         state         -8.80         -8.44         -0.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fp = false_positives[8]\n",
    "show_word_weights(fp)\n",
    "fn = false_negatives[6]\n",
    "show_word_weights(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"trump\" (also \"donald\") has a much larger weight for non-sarcastic labels.\n",
    "This complies with our analysis in milestone 1 regarding the most common lemmas: the word \"trump\" is way more frequent in non-sarcastics.\n",
    "This makes the Bag of Words model have a hard time detecting sarcastic headlines containing \"trump\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
