
# About the Datasets

## 1. Main Dataset
**Name**: News-Headlines-Dataset-For-Sarcasm-Detection  
**Authors**: R. Misra, A. Prahal, J. Grover  
**Source**: [GitHub](https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection)  
**File**: [Sarcasm_Headlines_Dataset.json](headline_data/Sarcasm_Headlines_Dataset.json)

### Description
The dataset contains around 28,000 entries collected from newspaper article headlines of two websites:  
- [*TheOnion*](https://www.theonion.com/)
- [*HuffPost*](https://www.huffingtonpost.com/)

Each headline is annotated with a binary class label indicating whether it is sarcastic. The labeling corresponds to the source of the article headline, where *TheOnion* produces sarcastic headlines, and *HuffPost* publishes serious news articles.

### Preprocessing
This folder contains the output of preprocessing applied to the raw data stored in the [headlines.conllu](headline_data/headlines.conllu) file, produced by the following script:

```bash
export DATA_DIR=data/headline_data
python src/preprocessing.py $DATA_DIR/Sarcasm_Headlines_Dataset.json $DATA_DIR/headlines.conllu
```

## 2. Tweets Dataset
**Name**: SemEval-2022 Task 6: iSarcasmEval, Intended Sarcasm Detection in English and Arabic  
**Authors**: I. Abu Farha, S. V. Oprea, S. Wilson, and W. Magdy  
**Source**: [GitHub](https://github.com/iabufarha/iSarcasmEval/blob/main/train/train.En.csv)

### Files
- [tweets.csv](tweets_data/tweets.csv) - Original raw data
- [tweets.json](tweets_data/tweets.json) - Raw data transformed into `.json` and sanitized using the `preprocess_tweets()` function from [preprocessing.py](../src/preprocessing.py)

### Description
This dataset contains tweets annotated for sarcasm detection, indicating whether the tweets are intentionally sarcastic or not. The dataset is imbalanced, with a 1:4 ratio between sarcastic and non-sarcastic instances.

### Preprocessing
The data was preprocessed using the script [preprocessing.py](../src/preprocessing.py) with the following command:

```bash
export DATA_DIR=data/tweets_data
python src/preprocessing.py $DATA_DIR/tweets.json $DATA_DIR/tweets.conllu
```

---

## 3. Generated Datasets

In addition to the datasets mentioned above, the following two datasets were generated for validation and control purposes:

### 3.1. Imitation of *The Onion* / *HuffPost* Dataset
**Purpose**: This dataset was generated by a language model to imitate the structure and content of the *TheOnion* / *HuffPost* dataset. It serves as a validation set for model performance, ensuring that the model can successfully identify sarcastic and non-sarcastic headlines that resemble real-world data.

**File**: [chatgpt_onionstyle.json](chatgpt_onionstyle_data/chatgpt_onionstyle.json)

### 3.2. Generic Sarcastic and Real Headlines (Control Data)
**Purpose**: This dataset consists of a balanced mix of generic sarcastic and real headlines, and functions as a control group for studies involving sarcasm detection. It is designed to provide a broader range of sarcastic and non-sarcastic data for comparison.

**File**: [chatgpt_generic.json](chatgpt_generic_data/chatgpt_generic.json)

## 4 Data Formats and Processing
All four datasets—**Sarcasm_Headlines_Dataset**, **Tweets**, **Imitated_Sarcasm_Headlines**, and **Control_Sarcasm_Headlines**—are stored in both **CONLL-U** and **JSON** formats. These datasets have been dependency parsed, and syntactic features were extracted from the parsed data. Both the dependency parsed data and the syntactic features are stored. The source files for each conversion can be found in the `src` folder:

- **`preprocessing.py`**: Converts raw JSON data into CONLL-U format.
- **`dependency_parsing.py`**: Parses the JSON data into a new JSON file containing syntax trees and POS tags.
- **`syntactic_feature_eng.py`**: Builds a CSV file containing syntactic features derived from the parsed data.

--- 
